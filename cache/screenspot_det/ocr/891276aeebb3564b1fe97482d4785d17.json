{"engine": "easyocr", "max_dim": 960, "lang": "eng", "psm": 11, "image_hw": {"h": 810, "w": 1440}, "words": [{"bbox_xywh": [370, 0, 462, 28], "text": "inaccuracies and biases present in the corpora 5", "conf": 0.47265158822026093}, {"bbox_xywh": [1066, 10, 234, 21], "text": "foedforward neural notwork", "conf": 0.5505083052289905}, {"bbox_xywh": [1018, 34, 270, 21], "text": "recurrent neural network lstm", "conf": 0.8632188957864978}, {"bbox_xywh": [1304, 34, 42, 21], "text": "gru", "conf": 0.9996260803419804}, {"bbox_xywh": [82, 62, 90, 24], "text": "contents", "conf": 0.9999637955856601}, {"bbox_xywh": [194, 62, 39, 21], "text": "hide", "conf": 0.8434173464775085}, {"bbox_xywh": [370, 52, 585, 27], "text": "notable examples include openals gpt models 0 g gpt 3 5", "conf": 0.8129435791705723}, {"bbox_xywh": [1070, 58, 39, 21], "text": "esn", "conf": 0.9998537472126787}, {"bbox_xywh": [1124, 58, 171, 24], "text": "reservolr computing", "conf": 0.8425716152668211}, {"bbox_xywh": [370, 86, 615, 27], "text": "and gpt 4 used in chatgpt google s palm used in bard and", "conf": 0.6145305093726889}, {"bbox_xywh": [1028, 82, 252, 21], "text": "restricted boltzmann machlne", "conf": 0.9335676621473551}, {"bbox_xywh": [1294, 82, 42, 21], "text": "gan", "conf": 0.9999865102111692}, {"bbox_xywh": [1088, 110, 126, 18], "text": "dlttusion model", "conf": 0.6115861537064557}, {"bbox_xywh": [1232, 106, 48, 21], "text": "som", "conf": 0.9994552659270977}, {"bbox_xywh": [82, 118, 57, 24], "text": "top", "conf": 0.9155666726381164}, {"bbox_xywh": [370, 116, 510, 24], "text": "meta s llama as well as bloom ernie 3 0 titan and", "conf": 0.43205432997185056}, {"bbox_xywh": [1028, 130, 309, 21], "text": "convolutional neural network u net", "conf": 0.7726708220429521}, {"bbox_xywh": [370, 148, 198, 24], "text": "anthropic s claude 2", "conf": 0.8998568012574589}, {"bbox_xywh": [1058, 154, 171, 21], "text": "transformer vision", "conf": 0.8296745124140332}, {"bbox_xywh": [1240, 158, 66, 18], "text": "mamba", "conf": 0.9999374114632005}, {"bbox_xywh": [81, 160, 74, 27], "text": "history", "conf": 0.9998659979509213}, {"bbox_xywh": [1018, 178, 114, 24], "text": "spiking neural", "conf": 0.9665724274818773}, {"bbox_xywh": [1138, 182, 72, 18], "text": "network", "conf": 0.8476718809303375}, {"bbox_xywh": [1222, 182, 123, 18], "text": "memtransistor", "conf": 0.6673875032152625}, {"bbox_xywh": [82, 206, 213, 27], "text": "dataset preprocessing", "conf": 0.955821497826131}, {"bbox_xywh": [364, 202, 110, 40], "text": "history", "conf": 0.8053827430094979}, {"bbox_xywh": [484, 214, 63, 24], "text": "edit", "conf": 0.49763154552733435}, {"bbox_xywh": [1059, 204, 129, 21], "text": "electrochemica", "conf": 0.9998340928764807}, {"bbox_xywh": [1196, 202, 123, 24], "text": "ram ecram", "conf": 0.9978743037953391}, {"bbox_xywh": [1058, 236, 210, 27], "text": "relnforcomont leaming", "conf": 0.4552837206283989}, {"bbox_xywh": [1310, 236, 60, 21], "text": "show", "conf": 0.7113572432819772}, {"bbox_xywh": [82, 250, 237, 24], "text": "training and architecture", "conf": 0.9934222109133648}, {"bbox_xywh": [370, 260, 48, 24], "text": "2017", "conf": 0.54944637765587}, {"bbox_xywh": [436, 260, 312, 24], "text": "google introduces transformer in", "conf": 0.691612076029544}, {"bbox_xywh": [764, 260, 210, 24], "text": "research paper which", "conf": 0.733009354451394}, {"bbox_xywh": [82, 278, 66, 24], "text": "details", "conf": 0.9799950706183392}, {"bbox_xywh": [1064, 278, 198, 21], "text": "leaming with humans", "conf": 0.738503187866123}, {"bbox_xywh": [1310, 272, 60, 21], "text": "show", "conf": 0.8148374456812403}, {"bbox_xywh": [370, 290, 582, 27], "text": "describes machine translation software using machine leaming", "conf": 0.6634116193765672}, {"bbox_xywh": [82, 322, 126, 24], "text": "training cost", "conf": 0.9505429845923017}, {"bbox_xywh": [368, 322, 108, 24], "text": "techniques", "conf": 0.6952973552873842}, {"bbox_xywh": [482, 322, 186, 21], "text": "the model size was", "conf": 0.723234503128487}, {"bbox_xywh": [686, 316, 237, 30], "text": "100 million parameters 6", "conf": 0.8198369947930606}, {"bbox_xywh": [1082, 314, 162, 24], "text": "model diagnostics", "conf": 0.5481370042437663}, {"bbox_xywh": [1310, 310, 60, 21], "text": "show", "conf": 0.5248661513708396}, {"bbox_xywh": [1046, 350, 234, 21], "text": "mathematical foundatlons", "conf": 0.7851796704879946}, {"bbox_xywh": [1310, 346, 60, 21], "text": "show", "conf": 0.7523861903258681}, {"bbox_xywh": [82, 368, 84, 21], "text": "tool use", "conf": 0.9997143539080926}, {"bbox_xywh": [368, 368, 51, 27], "text": "2018", "conf": 0.6046723917603463}, {"bbox_xywh": [436, 374, 72, 21], "text": "in june", "conf": 0.9971401898338942}, {"bbox_xywh": [518, 370, 309, 27], "text": "openai introduces generative pre", "conf": 0.5801178819533511}, {"bbox_xywh": [832, 374, 72, 21], "text": "trained", "conf": 0.9688882818131875}, {"bbox_xywh": [1048, 388, 228, 21], "text": "machina learing venues", "conf": 0.6312642049637051}, {"bbox_xywh": [1308, 381, 62, 27], "text": "show", "conf": 0.7299137575050457}, {"bbox_xywh": [370, 400, 579, 27], "text": "transformer gpt which introduces the concept of generating", "conf": 0.8512498373570718}, {"bbox_xywh": [82, 412, 75, 24], "text": "agency", "conf": 0.6332231607896265}, {"bbox_xywh": [1094, 424, 141, 21], "text": "rolatod articlos", "conf": 0.4258253992089668}, {"bbox_xywh": [1310, 422, 60, 21], "text": "show", "conf": 0.5650227914956883}, {"bbox_xywh": [368, 434, 609, 27], "text": "text using the transformer architecture and unsupervised learning", "conf": 0.7001758126576071}, {"bbox_xywh": [86, 458, 123, 21], "text": "compression", "conf": 0.9243517584929936}, {"bbox_xywh": [370, 466, 60, 21], "text": "in thls", "conf": 0.6150651972868598}, {"bbox_xywh": [429, 460, 296, 32], "text": "application 7 the gpt 1 modeb", "conf": 0.3795668551488114}, {"bbox_xywh": [730, 464, 138, 24], "text": "had 117 million", "conf": 0.7220472003092135}, {"bbox_xywh": [82, 500, 129, 24], "text": "multimodality", "conf": 0.9998379821936011}, {"bbox_xywh": [369, 495, 110, 26], "text": "parameters", "conf": 0.7021622481931286}, {"bbox_xywh": [484, 496, 333, 24], "text": "in october google introduced bert", "conf": 0.9053499730213328}, {"bbox_xywh": [818, 490, 100, 36], "text": "language", "conf": 0.998367636647185}, {"bbox_xywh": [368, 526, 594, 27], "text": "model which excelled at machine leaming translation tasks and", "conf": 0.6807711670606813}, {"bbox_xywh": [998, 526, 144, 27], "text": "larger bert had", "conf": 0.9500814842112664}, {"bbox_xywh": [1160, 526, 174, 24], "text": "slze of 340 million", "conf": 0.7697144655033465}, {"bbox_xywh": [82, 544, 102, 24], "text": "properties", "conf": 0.8903525801149428}, {"bbox_xywh": [369, 556, 110, 27], "text": "parameters", "conf": 0.8319164293496946}, {"bbox_xywh": [82, 590, 129, 24], "text": "interpretation", "conf": 0.8788387649017941}, {"bbox_xywh": [370, 608, 48, 24], "text": "2019", "conf": 0.9999992847442627}, {"bbox_xywh": [440, 608, 267, 24], "text": "openai releases gpt 2 with", "conf": 0.7750599319142988}, {"bbox_xywh": [724, 604, 591, 30], "text": "goal to scale up gpt 1 10 times 9 it had 1 5 billion parameters", "conf": 0.5811329097693618}, {"bbox_xywh": [82, 634, 102, 21], "text": "evaluation", "conf": 0.9999963433592445}, {"bbox_xywh": [370, 658, 54, 24], "text": "2020", "conf": 0.999984860420227}, {"bbox_xywh": [442, 658, 222, 27], "text": "openai releases gpt 3", "conf": 0.9352471543882296}, {"bbox_xywh": [712, 652, 237, 30], "text": "175 billion parameters 10", "conf": 0.9758010383903587}, {"bbox_xywh": [82, 676, 129, 24], "text": "wider impact", "conf": 0.9301124695190958}, {"bbox_xywh": [370, 710, 54, 24], "text": "2023", "conf": 0.9999974966049194}, {"bbox_xywh": [442, 710, 222, 24], "text": "openai releases gpt 4", "conf": 0.8354647429065826}, {"bbox_xywh": [728, 704, 315, 30], "text": "rumored 1 8 trillion parameters 11", "conf": 0.685544893022486}, {"bbox_xywh": [82, 724, 42, 21], "text": "list", "conf": 0.9999533295631409}, {"bbox_xywh": [366, 765, 300, 42], "text": "dataset preprocessing", "conf": 0.8837545756720474}, {"bbox_xywh": [676, 778, 60, 21], "text": "edit", "conf": 0.6722956630847832}, {"bbox_xywh": [958, 524, 40, 28], "text": "the", "conf": 0.9981909342315056}, {"bbox_xywh": [664, 656, 46, 28], "text": "with", "conf": 0.9998508316687824}, {"bbox_xywh": [664, 708, 46, 27], "text": "with", "conf": 0.9946608543395996}], "meta": {"available": true, "engine": "easyocr", "langs": ["en"], "gpu": false, "reader_cached": true, "words": 90}}